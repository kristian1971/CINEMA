{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Model train by edgeimpulse.com.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "VeQEgacnk08U"
      },
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, InputLayer, Dropout, Conv1D, Conv2D, Flatten, Reshape, MaxPooling1D, MaxPooling2D, BatchNormalization\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "sys.path.append('./resources/libraries')\n",
        "import ei_tensorflow.training\n",
        "\n",
        "# model architecture\n",
        "model = Sequential()\n",
        "model.add(Dense(20, activation='relu',\n",
        "    activity_regularizer=tf.keras.regularizers.l1(0.00001)))\n",
        "model.add(Dense(10, activation='relu',\n",
        "    activity_regularizer=tf.keras.regularizers.l1(0.00001)))\n",
        "model.add(Dense(classes, activation='softmax', name='y_pred'))\n",
        "\n",
        "# this controls the learning rate\n",
        "opt = Adam(lr=0.0005, beta_1=0.9, beta_2=0.999)\n",
        "# this controls the batch size, or you can manipulate the tf.data.Dataset objects yourself\n",
        "BATCH_SIZE = 32\n",
        "train_dataset, validation_dataset = ei_tensorflow.training.set_batch_size(BATCH_SIZE, train_dataset, validation_dataset)\n",
        "callbacks.append(BatchLoggerCallback(BATCH_SIZE, train_sample_count))\n",
        "\n",
        "# train the neural network\n",
        "model.compile(loss='categorical_crossentropy', optimizer=opt, metrics=['accuracy'])\n",
        "model.fit(train_dataset, epochs=30, validation_data=validation_dataset, verbose=2, callbacks=callbacks)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}